{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "of4dAyBENBa9",
        "1f9OEc5bNDye",
        "bpTkrG6TNhzn",
        "xzO8aiqfSuDe",
        "vc23jonUqTWU",
        "EVJHNov2zSKP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "of4dAyBENBa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awCg3i7BLAAO"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading & Setting Up Data"
      ],
      "metadata": {
        "id": "1f9OEc5bNDye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_extract():\n",
        "  if \"dataset\" in os.listdir():\n",
        "    print(\"Dataset already exists\")\n",
        "  else:\n",
        "    print(\"Downloading the data...\")\n",
        "    !wget -O food-data.zip https://www.kaggle.com/api/v1/datasets/download/trolukovich/food11-image-dataset\n",
        "    print(\"Dataset downloaded!\")\n",
        "    print(\"Extracting data..\")\n",
        "    !mkdir dataset\n",
        "    !unzip -q food-data.zip -d dataset\n",
        "    print(\"Extraction done!\")\n",
        "\n",
        "get_data_extract()"
      ],
      "metadata": {
        "id": "Z05RopOTMzu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ceda9f-0346-46bf-980f-39e861cf888a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the data...\n",
            "--2025-04-26 18:15:57--  https://www.kaggle.com/api/v1/datasets/download/trolukovich/food11-image-dataset\n",
            "Resolving www.kaggle.com (www.kaggle.com)... 35.244.233.98\n",
            "Connecting to www.kaggle.com (www.kaggle.com)|35.244.233.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com:443/kaggle-data-sets/432700/821742/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250426%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250426T181557Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2c954edd612b89a86a0884818a112b36531664d3a49da7e49118abdf02e9c4c1c50cd7c0941a692fa68eeeff93a9737d9afcb6e043acc899820a3e6ddaaeb567a35749e0c4341516db92222ff47f34460c8ad6218026ddd6985f769a73e29be16b666f8f5fe0b56fe1f736210544cbef7211f8ca02be9119ac5475c2d34c46a0c5439bc552e4e72840df76aac5a00249295a12e1618ec48bf541e0aed921be5af176509e4279d82ce3644bb6b7ebfabc12f5bc6038303e8ce15a0a2b80ebff03816c39ebadd09e7b73add9b129572c40a1c2214248237780eaea2a2a111f282357a081f6411ec2cd3814d68f41ae3f77797b6b12601acc17db609fd0b9c8a124 [following]\n",
            "--2025-04-26 18:15:57--  https://storage.googleapis.com/kaggle-data-sets/432700/821742/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250426%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250426T181557Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2c954edd612b89a86a0884818a112b36531664d3a49da7e49118abdf02e9c4c1c50cd7c0941a692fa68eeeff93a9737d9afcb6e043acc899820a3e6ddaaeb567a35749e0c4341516db92222ff47f34460c8ad6218026ddd6985f769a73e29be16b666f8f5fe0b56fe1f736210544cbef7211f8ca02be9119ac5475c2d34c46a0c5439bc552e4e72840df76aac5a00249295a12e1618ec48bf541e0aed921be5af176509e4279d82ce3644bb6b7ebfabc12f5bc6038303e8ce15a0a2b80ebff03816c39ebadd09e7b73add9b129572c40a1c2214248237780eaea2a2a111f282357a081f6411ec2cd3814d68f41ae3f77797b6b12601acc17db609fd0b9c8a124\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.207, 142.250.101.207, 142.250.141.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1163229310 (1.1G) [application/zip]\n",
            "Saving to: ‘food-data.zip’\n",
            "\n",
            "food-data.zip       100%[===================>]   1.08G   146MB/s    in 15s     \n",
            "\n",
            "2025-04-26 18:16:12 (75.6 MB/s) - ‘food-data.zip’ saved [1163229310/1163229310]\n",
            "\n",
            "Dataset downloaded!\n",
            "Extracting data..\n",
            "Extraction done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "path_train = glob.glob('dataset/training/*/*.jpg')\n",
        "label_train = [i.split(\".\")[0].split(\"/\")[-2] for i in path_train]\n",
        "\n",
        "# Validation set\n",
        "path_val = glob.glob('dataset/validation/*/*.jpg')\n",
        "label_val = [i.split(\".\")[0].split(\"/\")[-2] for i in path_val]\n",
        "\n",
        "# Evaluation set\n",
        "path_eval = glob.glob('dataset/evaluation/*/*.jpg')\n",
        "label_eval = [i.split(\".\")[0].split(\"/\")[-2] for i in path_eval]\n",
        "\n",
        "print(f\"Train: {len(path_train)} images, {len(label_train)} labels\")\n",
        "print(f\"Validation: {len(path_val)} images, {len(label_val)} labels\")\n",
        "print(f\"Evaluation: {len(path_eval)} images, {len(label_eval)} labels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdfwwEwQM2rt",
        "outputId": "f1f3428c-e76a-4a48-c42a-0a3d4b674b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 9866 images, 9866 labels\n",
            "Validation: 3430 images, 3430 labels\n",
            "Evaluation: 3347 images, 3347 labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7PQB6MiNIIn",
        "outputId": "ab193463-4b29-4f48-c00c-96f6a06ee52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Egg', 'Egg', 'Egg', 'Egg', 'Egg']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(label_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wb-j8bSNJ0_",
        "outputId": "37001815-864e-4db2-c910-3c02eb3665c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bread',\n",
              " 'Dairy product',\n",
              " 'Dessert',\n",
              " 'Egg',\n",
              " 'Fried food',\n",
              " 'Meat',\n",
              " 'Noodles-Pasta',\n",
              " 'Rice',\n",
              " 'Seafood',\n",
              " 'Soup',\n",
              " 'Vegetable-Fruit'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 244)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root='dataset/training', transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root='dataset/validation', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='dataset/evaluation', transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "_QtoECkfNLKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ImageFolder automatically assigns a label to each image based on the folder name."
      ],
      "metadata": {
        "id": "2pt3nL9sNPM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rInp8AugNNf_",
        "outputId": "87aa9c18-f6b9-4a94-e206-743d0d6978fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 9866\n",
              "    Root location: dataset/training\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(224, 244), interpolation=bilinear, max_size=None, antialias=True)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# batch is a tuple: (inputs, labels)\n",
        "inputs, labels = batch\n",
        "\n",
        "print(labels.shape)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YZUH7jONPwA",
        "outputId": "7f223bc0-124a-4cac-a160-35708f8fd0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32])\n",
            "tensor([ 3,  4, 10,  8,  5,  2,  3,  9,  2,  5,  1, 10,  3,  8,  9,  9,  4,  0,\n",
            "         9,  5,  0,  9,  9,  0,  9,  5,  3,  3,  0,  3,  4,  2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QodRxwLzNU1m",
        "outputId": "d07b2ce0-bb35-4958-81f4-e3527e63f8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 3430\n",
              "    Root location: dataset/validation\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(224, 244), interpolation=bilinear, max_size=None, antialias=True)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A113cBK8NWXS",
        "outputId": "632bf2c2-edf0-4575-a699-bde892610ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 3347\n",
              "    Root location: dataset/evaluation\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=(224, 244), interpolation=bilinear, max_size=None, antialias=True)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting three images from each\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "val_features, val_labels = next(iter(val_loader))\n",
        "test_features, test_labels = next(iter(test_loader))\n",
        "\n",
        "def show_images(features, labels, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i in range(3):\n",
        "        img = features[i].permute(1, 2, 0)\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Label: {labels[i].item()}\")\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "show_images(train_features, train_labels, \"Training Samples\")\n",
        "show_images(val_features, val_labels, \"Validation Samples\")\n",
        "show_images(test_features, test_labels, \"Evaluation Samples\")"
      ],
      "metadata": {
        "id": "LofO1lbbNXsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "2HrZbM5MNdOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load EfficientNet (pretrained)\n",
        "2. Replace classification head\n",
        "3. For feature extraction: freeze all base layers\n",
        "4. For fine-tuning: unfreeze last n layers or progressively unfreeze\n",
        "5. Train, validate, and plot metrics"
      ],
      "metadata": {
        "id": "IQdZEej9Nfk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training only HEAD of EfficientNet"
      ],
      "metadata": {
        "id": "bpTkrG6TNhzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Image --> EfficientNet Feature Extractor --> (Dropout) --> Linear(1280 -> Our classes)"
      ],
      "metadata": {
        "id": "3GTcP65tNlDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained EfficientNet\n",
        "model = models.efficientnet_b0(pretrained=True)"
      ],
      "metadata": {
        "id": "sJKRxc_hNhV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "3nmy6XDwNmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "id": "q-3lIwh0NpCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier[1].in_features"
      ],
      "metadata": {
        "id": "45cDZ1iANqfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- EfficientNet is used for 1000 Classes. We need to change this to match our task."
      ],
      "metadata": {
        "id": "OuwVk8ynN82t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the classifier head to match our classes\n",
        "num_classes = len(set(label_train))\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)"
      ],
      "metadata": {
        "id": "NssqwlpXN_Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "id": "pXTNpWXrOBGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.classifier[1].parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "KsovexsYNsHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNet Freezed - HEAD + CNN + MLP"
      ],
      "metadata": {
        "id": "xzO8aiqfSuDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained EfficientNet\n",
        "model = models.efficientnet_b0(pretrained=True)"
      ],
      "metadata": {
        "id": "tnr7Q0mnWJRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ca54dd-bf17-418f-89ff-48567554cfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 143MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, efficientnet_backbone, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.backbone = efficientnet_backbone\n",
        "        self.backbone.classifier = nn.Identity()  # Remove EfficientNet head\n",
        "\n",
        "        # Freeze EfficientNet\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.extra_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(1280 + 64, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features_effnet = self.backbone(x)  # [batch_size, 1280]\n",
        "        features_cnn = self.extra_cnn(x)    # [batch_size, 64]\n",
        "        combined = torch.cat((features_effnet, features_cnn), dim=1)  # [batch_size, 1344]\n",
        "        output = self.mlp(combined)\n",
        "        return output"
      ],
      "metadata": {
        "id": "7lDtLq5xWNbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define model\n",
        "num_classes = 11\n",
        "model = HybridModel(efficientnet_backbone=model, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (train only the new parts)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "# Training settings\n",
        "epochs = 10\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "lju2ylxWTS5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNet Non Freezed - HEAD + CNN + MLP"
      ],
      "metadata": {
        "id": "-1RqPOKQcrJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Uses different learning rates (small for EfficientNet, normal for CNN+MLP)\n"
      ],
      "metadata": {
        "id": "FAzuXKT3c4QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_model = models.efficientnet_b0(weights='IMAGENET1K_V1')"
      ],
      "metadata": {
        "id": "5lBRdfbZdLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, efficientnet_backbone, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.backbone = efficientnet_backbone\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        # Freeze EfficientNet\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.extra_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(1280 + 64, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features_effnet = self.backbone(x)\n",
        "        features_cnn = self.extra_cnn(x)\n",
        "        combined = torch.cat((features_effnet, features_cnn), dim=1)\n",
        "        output = self.mlp(combined)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XLYjQWGdMkut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "metadata": {
        "id": "Uowq2jrMdNO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "with mlflow.start_run():\n",
        "\n",
        "    # Define model\n",
        "    num_classes = 11\n",
        "    model = HybridModel(efficientnet_backbone=efficientnet_model, num_classes=num_classes)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # === Unfreeze EfficientNet backbone ===\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # === Define optimizer with differential learning rates ===\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.extra_cnn.parameters(), 'lr': 1e-3},\n",
        "        {'params': model.mlp.parameters(), 'lr': 1e-3},\n",
        "    ])\n",
        "\n",
        "    # === Define loss function and scheduler ===\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    # === Training settings ===\n",
        "    epochs = 10\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # ✅ Log metrics for every epoch\n",
        "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
        "\n",
        "    # --- Test Set Evaluation ---\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = 100 * test_correct / test_total\n",
        "\n",
        "    print(f\"\\n✅ Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # ✅ Save and log model\n",
        "    torch.save(model.state_dict(), \"model.pth\")\n",
        "    mlflow.pytorch.log_model(model, \"model\")\n",
        "\n",
        "    # ✅ Log final test metrics\n",
        "    mlflow.log_metric(\"test_loss\", test_loss)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
        "\n",
        "    # ✅ Log hyperparameters\n",
        "    mlflow.log_param(\"epochs\", epochs)\n",
        "    mlflow.log_param(\"batch_size\", 32)\n",
        "    mlflow.log_param(\"lr_backbone\", 1e-5)\n",
        "    mlflow.log_param(\"lr_heads\", 1e-3)"
      ],
      "metadata": {
        "id": "B7XRA0ULT36r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b22a49-edc6-49f0-f7c2-c79886eee9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] | Train Loss: 0.5634 | Train Acc: 81.54% | Val Loss: 0.4526 | Val Acc: 86.41%\n",
            "Epoch [2/10] | Train Loss: 0.2729 | Train Acc: 90.71% | Val Loss: 0.4096 | Val Acc: 88.22%\n",
            "Epoch [3/10] | Train Loss: 0.1885 | Train Acc: 93.78% | Val Loss: 0.4155 | Val Acc: 87.84%\n",
            "Epoch [4/10] | Train Loss: 0.1394 | Train Acc: 95.36% | Val Loss: 0.4647 | Val Acc: 88.22%\n",
            "Epoch [5/10] | Train Loss: 0.1160 | Train Acc: 96.04% | Val Loss: 0.4815 | Val Acc: 88.02%\n",
            "Epoch [6/10] | Train Loss: 0.0818 | Train Acc: 97.36% | Val Loss: 0.4535 | Val Acc: 88.89%\n",
            "Epoch [7/10] | Train Loss: 0.0706 | Train Acc: 97.70% | Val Loss: 0.4409 | Val Acc: 89.10%\n",
            "Epoch [8/10] | Train Loss: 0.0506 | Train Acc: 98.36% | Val Loss: 0.4780 | Val Acc: 89.39%\n",
            "Epoch [9/10] | Train Loss: 0.0520 | Train Acc: 98.40% | Val Loss: 0.4950 | Val Acc: 89.30%\n",
            "Epoch [10/10] | Train Loss: 0.0451 | Train Acc: 98.60% | Val Loss: 0.4859 | Val Acc: 88.98%\n",
            "\n",
            "✅ Test Loss: 0.3920 | Test Accuracy: 91.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/04/26 19:20:32 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/04/26 19:20:43 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.21.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torchvision==0.21.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "\u001b[31m2025/04/26 19:20:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradual Freezing + Scheduler"
      ],
      "metadata": {
        "id": "vc23jonUqTWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load EfficientNet backbone\n",
        "efficientnet_model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Instantiate model\n",
        "num_classes = 11\n",
        "model = HybridModel(efficientnet_backbone=efficientnet_model, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# --- Gradual Unfreezing ---\n",
        "for name, param in model.backbone.named_parameters():\n",
        "    if 'features.6' in name or 'features.7' in name or 'features.8' in name:\n",
        "        param.requires_grad = True  # Unfreeze only last few layers\n",
        "    else:\n",
        "        param.requires_grad = False  # Freeze early layers\n",
        "\n",
        "# --- Loss and Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # label smoothing added\n",
        "\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
        "    {'params': model.extra_cnn.parameters(), 'lr': 1e-3},\n",
        "    {'params': model.mlp.parameters(), 'lr': 1e-3},\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# --- Scheduler (optional) ---\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Step scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "6HTEkxPddTn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the mlruns folder\n",
        "!zip -r mlruns.zip mlruns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA_1dR_YkCeD",
        "outputId": "d1ce013c-1b5b-4f65-aea8-460f90816977"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: mlruns/ (stored 0%)\n",
            "  adding: mlruns/.trash/ (stored 0%)\n",
            "  adding: mlruns/0/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/params/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/params/lr_heads (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/params/lr_backbone (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/params/epochs (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/params/batch_size (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/meta.yaml (deflated 43%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/train_accuracy (deflated 51%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/test_loss (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/val_accuracy (deflated 53%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/test_accuracy (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/train_loss (deflated 52%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/metrics/val_loss (deflated 52%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/MLmodel (deflated 44%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/python_env.yaml (deflated 17%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/conda.yaml (deflated 40%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/data/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/data/pickle_module_info.txt (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/data/model.pth (deflated 8%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/artifacts/model/requirements.txt (deflated 31%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/tags/ (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/tags/mlflow.source.type (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/tags/mlflow.runName (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/tags/mlflow.user (stored 0%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/tags/mlflow.log-model.history (deflated 43%)\n",
            "  adding: mlruns/0/9602c2bfff504895a9386b39f1a7bdab/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: mlruns/0/meta.yaml (deflated 24%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_LLJ0nN2zQsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stronger Model B4"
      ],
      "metadata": {
        "id": "EVJHNov2zSKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct transforms for EfficientNet-B3\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root='dataset/training', transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root='dataset/validation', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='dataset/evaluation', transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "NQguUP7h1tnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, efficientnet_backbone, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.backbone = efficientnet_backbone\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.extra_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Create a dummy input to pass through backbone to get output size\n",
        "        dummy_input = torch.zeros(1, 3, 300, 300)\n",
        "        with torch.no_grad():\n",
        "            backbone_output = self.backbone(dummy_input)\n",
        "            backbone_features = backbone_output.shape[1]\n",
        "\n",
        "        extra_cnn_features = 64\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(backbone_features + extra_cnn_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features_effnet = self.backbone(x)\n",
        "        features_cnn = self.extra_cnn(x)\n",
        "        combined = torch.cat((features_effnet, features_cnn), dim=1)\n",
        "        output = self.mlp(combined)\n",
        "        return output"
      ],
      "metadata": {
        "id": "HdnsJNrg2Fid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Setup ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load EfficientNet-B3\n",
        "efficientnet_model = models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Define model\n",
        "num_classes = 11\n",
        "model = HybridModel(efficientnet_backbone=efficientnet_model, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (train all parameters)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "# --- Training ---\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "dicsl_U_zUc5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}